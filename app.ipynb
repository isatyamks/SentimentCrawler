{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(url):\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract the main heading (in <h1>)\n",
    "        main_heading = soup.find('h1').get_text(strip=True) if soup.find('h1') else ''\n",
    "        \n",
    "        # Extract subheadings (in <strong>)\n",
    "        subheadings = soup.find_all('strong')\n",
    "        subheading_text = '\\n'.join([sub.get_text(strip=True) for sub in subheadings])\n",
    "        \n",
    "        # Extract body content (in <p>)\n",
    "        body_paragraphs = soup.find_all('p')\n",
    "        body_text = '\\n'.join([p.get_text(strip=True) for p in body_paragraphs])\n",
    "        # Combine all parts into a single article text\n",
    "        article_text = f\"{main_heading}\\n\\n{subheading_text}\\n\\n{body_text}\"\n",
    "        # print(article_text)\n",
    "        return article_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching article from {url}: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(url, article_text, save_dir):\n",
    "    try:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        url_suffix = url.split(\"blackcoffer.com/\")[-1]\n",
    "        safe_filename = re.sub(r'[<>:\"/\\\\|?*]', '_', url_suffix) \n",
    "        filename = os.path.join(save_dir, f\"{safe_filename}.txt\")  \n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(article_text)\n",
    "        print(f\"Saved article to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 147 entries, 0 to 146\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   URL_ID  147 non-null    object\n",
      " 1    URL    147 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netclan20241017</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-ml-ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netclan20241018</td>\n",
       "      <td>https://insights.blackcoffer.com/enhancing-fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netclan20241019</td>\n",
       "      <td>https://insights.blackcoffer.com/roas-dashboa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netclan20241020\\t</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netclan20241021\\t</td>\n",
       "      <td>https://insights.blackcoffer.com/development-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              URL_ID                                                URL\n",
       "0   Netclan20241017    https://insights.blackcoffer.com/ai-and-ml-ba...\n",
       "1   Netclan20241018    https://insights.blackcoffer.com/enhancing-fr...\n",
       "2   Netclan20241019    https://insights.blackcoffer.com/roas-dashboa...\n",
       "3  Netclan20241020\\t   https://insights.blackcoffer.com/efficient-pr...\n",
       "4  Netclan20241021\\t   https://insights.blackcoffer.com/development-..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Netclan20241017 \n",
       "1       Netclan20241018 \n",
       "2       Netclan20241019 \n",
       "3      Netclan20241020\\t\n",
       "4      Netclan20241021\\t\n",
       "             ...        \n",
       "142    Netclan20241159\\t\n",
       "143    Netclan20241160\\t\n",
       "144    Netclan20241161\\t\n",
       "145    Netclan20241162\\t\n",
       "146    Netclan20241163\\t\n",
       "Name: URL_ID, Length: 147, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.URL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Netclan20241017 \n",
      "1       Netclan20241018 \n",
      "2       Netclan20241019 \n",
      "3      Netclan20241020\\t\n",
      "4      Netclan20241021\\t\n",
      "             ...        \n",
      "142    Netclan20241159\\t\n",
      "143    Netclan20241160\\t\n",
      "144    Netclan20241161\\t\n",
      "145    Netclan20241162\\t\n",
      "146    Netclan20241163\\t\n",
      "Name: URL_ID, Length: 147, dtype: object\n",
      "0       Netclan20241017 \n",
      "1       Netclan20241018 \n",
      "2       Netclan20241019 \n",
      "3      Netclan20241020\\t\n",
      "4      Netclan20241021\\t\n",
      "             ...        \n",
      "142    Netclan20241159\\t\n",
      "143    Netclan20241160\\t\n",
      "144    Netclan20241161\\t\n",
      "145    Netclan20241162\\t\n",
      "146    Netclan20241163\\t\n",
      "Name: URL_ID, Length: 147, dtype: object\n",
      "0       Netclan20241017 \n",
      "1       Netclan20241018 \n",
      "2       Netclan20241019 \n",
      "3      Netclan20241020\\t\n",
      "4      Netclan20241021\\t\n",
      "             ...        \n",
      "142    Netclan20241159\\t\n",
      "143    Netclan20241160\\t\n",
      "144    Netclan20241161\\t\n",
      "145    Netclan20241162\\t\n",
      "146    Netclan20241163\\t\n",
      "Name: URL_ID, Length: 147, dtype: object\n",
      "0       Netclan20241017 \n",
      "1       Netclan20241018 \n",
      "2       Netclan20241019 \n",
      "3      Netclan20241020\\t\n",
      "4      Netclan20241021\\t\n",
      "             ...        \n",
      "142    Netclan20241159\\t\n",
      "143    Netclan20241160\\t\n",
      "144    Netclan20241161\\t\n",
      "145    Netclan20241162\\t\n",
      "146    Netclan20241163\\t\n",
      "Name: URL_ID, Length: 147, dtype: object\n",
      "0       Netclan20241017 \n",
      "1       Netclan20241018 \n",
      "2       Netclan20241019 \n",
      "3      Netclan20241020\\t\n",
      "4      Netclan20241021\\t\n",
      "             ...        \n",
      "142    Netclan20241159\\t\n",
      "143    Netclan20241160\\t\n",
      "144    Netclan20241161\\t\n",
      "145    Netclan20241162\\t\n",
      "146    Netclan20241163\\t\n",
      "Name: URL_ID, Length: 147, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(data.URL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = extract_article(url_id)\n",
    "save_dir = \"articles\"\n",
    "save_to_file(url_id, article_text, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
